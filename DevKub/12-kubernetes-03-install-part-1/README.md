# Домашнее задание к занятию "12.3 Развертывание кластера на собственных серверах, лекция 1"
Поработав с персональным кластером, можно заняться проектами. Вам пришла задача подготовить кластер под новый проект.

## Задание 1: Описать требования к кластеру
Сначала проекту необходимо определить требуемые ресурсы. Известно, что проекту нужны база данных, система кеширования, а само приложение состоит из бекенда и фронтенда. Опишите, какие ресурсы нужны, если известно:

* База данных должна быть отказоустойчивой. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии.
* Кэш должен быть отказоустойчивый. Потребляет 4 ГБ ОЗУ в работе, 1 ядро. 3 копии.
* Фронтенд обрабатывает внешние запросы быстро, отдавая статику. Потребляет не более 50 МБ ОЗУ на каждый экземпляр, 0.2 ядра. 5 копий.
* Бекенд потребляет 600 МБ ОЗУ и по 1 ядру на копию. 10 копий.

## Как оформить ДЗ?

Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.

План расчета
1. Сначала сделайте расчет всех необходимых ресурсов.

-----------------

Кластер включает в себя control plane (Master Node) и Data Plane (Worker Node)

    Master Node (мастер-нода) — узел, который управляет всем кластером. Он следит за остальными нодами и распределяет между ними нагрузку. Как правило, мастер-нода занимается только управлением и не берет на себя никакие рабочие нагрузки. Для повышения отказоустойчивости мастер-нод должно быть несколько.
    Worker Node (рабочие ноды) — узлы, на которых и работают контейнеры. На одном узле может работать много контейнеров в зависимости от параметров ноды (объем памяти и CPU) и требований контейнера

Рабочих узлов обычно больше, чем мастер-нод. Потенциально чем их больше, тем большее количество приложений можно запустить и тем отказоустойчивее будет кластер, потому что в случае выхода из строя одной ноды нагрузка переносится на другие.

Количество Master Node должно быть нечётным, минимально состоит из одной ноды. Количество и параметры (CPU/RAM/HDD) Worker Node влияют на возможность запуска контейнеров, в случае если наша нода не будет соответсвовать по памяти либо CPU запускаемому приложению - оно не сможет запуститься и стабильно работать.

При построении кластера можно обойтись одной-двумя мощными Worker Node, однако в этом случае надёжность кластера не будет реализована в полной мере.

При проектировании следует уделить внимание БД - при развёртывании в кластере неизбежно влияние рядом развёрнутых приложений на работу БД, т.к. выделяется процессорное время - а не сам процессор, возможно лучшим решением будет развёртывание БД на отдельных серверах. 
Так же к БД предъявляется требование по высокой доступности и управлению приложением (Stateful), т.е. хранить своё состояние, а это дополнительная нагрузка на Master Node. Так же для БД критичен механизм PodAntiAffinyty - для того что бы Pod's не размещались на одних и тех же серверах или нодах. 
PodDisruptionBudget - так же влияет на высокую доступность БД, позволяя указать количество недоступных нод в единый момент.

Кэш - для кэша, как и для БД предъявляются такие же требования по доступности и размещению Pod's на нодах. 

Фронтенд - может размещаться на любой из нод, фактически является классическим Stateless- приложением. 
Бэкэнд -  в зависисмости от реализуемых функций и логики может быть как Stateless так и Statefull приложением. 

Для реализации высокой доступности Фронтенд и Бэкенд основным является минимальное количество запущенных экземпляров, размещаться они могут как на одной ноде, так и на нескольких.

Так же при расчёте CPU/RAM необходимо учитывать требования системы на которой будут подняты наши ноды, для простоты расчёта возмём минимальные требования для работы Ubuntu - 1 Ghz CPU/ 512 Mb RAM/ 2.5 GB Hdd

Попробуем свести имеющиеся данные для расчёта 
БД  - 4 ГБ ОЗУ, 1 ядро. 3 копии, отказоустойчивость
Кэш - 4 ГБ ОЗУ, 1 ядро. 3 копии, отказоустойчивость
Фронтенд - 50 МБ ОЗУ, 0.2 ядра. 5 копий.
Бекенд - 600 МБ ОЗУ, 1 ядро. 10 копий.


| Компонента  |  Ram  | CPU | Replicas | SummRAM | Summ CPU |
| ----------- | ------| --- | -------- | ------- | -------- |
| База данных |   4   |  1  |    3     |   12    |    3     |
| Кэш         |   4   |  1  |    3     |   12    |    3     |
| Фронтэнд    |  0.05 | 0.2 |    5     |   0.25  |    1     |
| Бэкэнд      |  0.6  |  1  |   10     |   6     |    10    |
| ----------- | ----- | --- | -------- | ------- | -------- |

В задании не указан размер дискового пространства необходимого для каждой из компонент, для простоты расчёта введены следующие ограничения на диски:
    БД, Кэш: 20 Гб
    Фронт, Бэк: 2 Гб


С учётом вышесказанного и предъявляемых требований к БД, Кэш, фронтэнд, бэкэнд можно привести следующий расчёт для одной ноды:

    CPU = CPU_БД + CPU_К + CPU_Ф*replic + CPU_Б*replic = 1 + 1 + 0.2*5 + 1*10 =  13 CPU 
    RAM = R_БД + R_К + R_Ф*replicas + R_Б*replicas = 4 + 4 + 0.05*5 + 0.6*10 = 14.25 ГБ ~ 15 ГБ - округляем в большую сторону, с учётом требований ОС где будет развёрнута нода
    HDD = HDD_БД + HDD_К + HDD_Ф*replic + HDD_Б*replic = 20 + 20 + 2*5 + 2*10 =  70 Gb 


2. Затем прикиньте количество рабочих нод, которые справятся с такой нагрузкой.

С учётом требования к отказоустойчивости БД и КЭШ и необходимости разнесения экземпляров pod по разным нодам, минимальное количество Worker Node принимается равным трём - это позволит реализовать отказоустойчивость.

3. Добавьте к полученным цифрам запас, который учитывает выход из строя как минимум одной ноды.

Ориентируясь на расчёт из п.1 и учитывая необходимый запас для поднятия недостающих нод, а так же необходимость поддержания нод в рабочем состояннии в процессе обновления введём коэфициент резервирования равный 50% от расчётных ресурсов ноды.

получим и округлим до ближайшего целого/разумного значения
    RAM * 1.5 = 22.5 Gb ~ 24 гб
    CPU * 1.5 = 19.5 CPUs ~ 20
    HDD * 1.5 = 105 Gb ~ 110

4. Добавьте служебные ресурсы к нодам. Помните, что для разных типов нод требовния к ресурсам разные.

К служебным ресурсам можно отнести Master Node - требования по CPU/RAM/HDD для которых не так критичны 

Ресурс | 1 нода |
------ | ------ |
 CPU   |  2     |
 RAM   |  2 Gb  |
 HDD   |  50 Гб |

5. Рассчитайте итоговые цифры.

Ресурс  | Worker Node | к-во нод | Master Node |  к-во нод |
--------|------------ | -------- | ----------- |---------- |
**CPU** |    20       |    3     |    2        |     1     |
**RAM** |    24 Gb    |    3     |    2        |     1     |
**HDD** |    110 Gb   |    3     |    50 Gb    |     1     |

### Минимально требуемые ресурсы*
#### CPU: 62
#### RAM: 74
#### HDD: 380 

*цифры указывают на обшее количество требуемых ресурсов для поднятия кластера без учёта разбиения на отдельные сервера.

6. В результате должно быть указано количество нод и их параметры.

Ресурс  | Worker Node | к-во нод | Master Node |  к-во нод |
--------|------------ | -------- | ----------- |---------- |
**CPU** |    20       |    3     |    2        |     1     |
**RAM** |    24 Gb    |    3     |    2        |     1     |
**HDD** |    110 Gb   |    3     |    50 Gb    |     1     |

*Для повышения надёжности необходимо увеличить количество Master node c одной до 3-х, так же можно добавить дополнительную Master Node, однако следует учитывать что в случае использования 4-х Master Node ресурсы одной из них будут не доутилизированы.



----

Дополнительные материалы и ресурсы используемые при проведении расчётов:

[Как работает CPU Manager в Kubernetes](https://habr.com/ru/company/flant/blog/418269/)

[Рабочие узлы Kubernetes: много маленьких или несколько больших?](https://mcs.mail.ru/blog/rabochie-uzly-kubernetes-mnogo-malenkih-ili-neskolko-bolshih)

[Компоненты Kubernetes](https://kubernetes.io/ru/docs/concepts/overview/components/)

[KUBERNETES INSTANCE CALCULATOR](https://learnk8s.io/kubernetes-instance-calculator)

[Внутреннее устройство Kubernetes-кластера простым языком](https://habr.com/ru/company/flant/blog/583660/)

[Кластер PostgreSQL внутри Kubernetes: что нужно знать для успешного внедрения](https://habr.com/ru/company/raiffeisenbank/blog/539790/)
